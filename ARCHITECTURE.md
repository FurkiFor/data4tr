# ğŸ—ï¸ data4tr - Mimari DokÃ¼mantasyonu

## Genel BakÄ±ÅŸ

data4tr, **modÃ¼ler**, **konfigÃ¼rasyon tabanlÄ±** ve **geniÅŸletilebilir** bir mimari ile tasarlanmÄ±ÅŸtÄ±r.

## Mimari Prensipleri

### 1. **Configuration-Based Architecture**
- TÃ¼m URL'ler, ayarlar ve konfigÃ¼rasyonlar `config/config.yaml` dosyasÄ±nda
- Hardcoded deÄŸerler yok
- Environment variable desteÄŸi

### 2. **Modular Design**
- Her modÃ¼l baÄŸÄ±msÄ±z Ã§alÄ±ÅŸabilir
- Loose coupling, high cohesion
- Interface-based design

### 3. **Pattern-Based Architecture**
- **Factory Pattern**: Scraper oluÅŸturma
- **Registry Pattern**: Scraper kaydÄ±
- **Abstract Base Classes**: BaseScraper, BaseProcessor
- **Singleton Pattern**: ConfigManager

## Proje YapÄ±sÄ±

```
data4tr/
â”œâ”€â”€ config/              # KonfigÃ¼rasyon YÃ¶netimi
â”‚   â”œâ”€â”€ config.yaml     # TÃ¼m ayarlar burada
â”‚   â””â”€â”€ config.py       # ConfigManager (Singleton)
â”‚
â”œâ”€â”€ scraper/            # Veri Toplama ModÃ¼lÃ¼
â”‚   â”œâ”€â”€ base.py        # BaseScraper abstract class
â”‚   â”œâ”€â”€ scraper.py     # Ana scraper orkestratÃ¶rÃ¼
â”‚   â”œâ”€â”€ cleaner.py     # Text cleaning utilities
â”‚   â””â”€â”€ sources/       # Kaynak-specific scraper'lar
â”‚       â”œâ”€â”€ wikipedia.py
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ processor/         # Ä°ÅŸleme ModÃ¼lÃ¼
â”‚   â”œâ”€â”€ classify.py    # SÄ±nÄ±flandÄ±rma
â”‚   â”œâ”€â”€ deduplicate.py # Duplicate removal
â”‚   â””â”€â”€ normalize.py   # Text normalization
â”‚
â”œâ”€â”€ exporter/          # DÄ±ÅŸa Aktarma ModÃ¼lÃ¼
â”‚   â”œâ”€â”€ export_jsonl.py
â”‚   â”œâ”€â”€ export_csv.py
â”‚   â””â”€â”€ schema.json
â”‚
â”œâ”€â”€ algorithms/       # Matematiksel Algoritmalar
â”‚   â””â”€â”€ metrics.py   # Metin metrikleri ve kalite skorlarÄ±
â”‚
â””â”€â”€ cli.py           # Command Line Interface
```

## Katmanlar

### Configuration Layer
```python
from config import get_config

config = get_config()
api_url = config.get('sources.wikipedia.api.base_url')
```

**Ã–zellikler:**
- YAML tabanlÄ± konfigÃ¼rasyon
- Environment variable desteÄŸi `${VAR_NAME}`
- Nested key eriÅŸimi (`sources.wikipedia.api.base_url`)
- Singleton pattern ile global eriÅŸim

### Scraper Layer
```python
from scraper.scraper import Scraper
from scraper.sources.wikipedia import WikipediaScraper
from scraper.base import ScraperRegistry, BaseScraper
```

**Mimari:**
- `BaseScraper`: TÃ¼m scraper'lar iÃ§in abstract base class
- `ScraperRegistry`: Factory pattern ile scraper oluÅŸturma
- Kaynak-specific scraper'lar `sources/` altÄ±nda
- KonfigÃ¼rasyondan otomatik olarak bilgi alÄ±r

**Yeni Scraper Ekleme:**
```python
# 1. Scraper oluÅŸtur
class MySourceScraper(BaseScraper):
    def validate_config(self):
        # Config validasyonu
        return True
    
    def scrape(self, limit):
        # Scraping logic
        return data

# 2. Registry'ye kaydet
ScraperRegistry.register('mysource', MySourceScraper)

# 3. config.yaml'a ekle
# sources:
#   mysource:
#     enabled: true
#     api:
#       base_url: "https://api.example.com"
```

### Processing Layer
- Modular processors
- Her processor baÄŸÄ±msÄ±z Ã§alÄ±ÅŸabilir
- Pipeline pattern ile birleÅŸtirilebilir

### Export Layer
- Format-specific exporters
- Schema validation
- Configurable output

## KonfigÃ¼rasyon YÃ¶netimi

### YAML KonfigÃ¼rasyonu
```yaml
sources:
  wikipedia:
    enabled: true
    api:
      base_url: "https://tr.wikipedia.org/api/rest_v1"
      random_page: "/page/random/summary"
      timeout: 10
    rate_limit:
      delay_between_requests: 1.0
```

### Environment Variables
```yaml
sources:
  mysource:
    api:
      api_key: "${MY_API_KEY}"  # Environment'tan alÄ±nÄ±r
```

### Programmatic EriÅŸim
```python
config = get_config()

# Nested key eriÅŸimi
url = config.get('sources.wikipedia.api.base_url')

# Tam config objesi
source_config = config.get_source_config('wikipedia')

# Ayar deÄŸiÅŸtirme
config.set('sources.wikipedia.enabled', False)
```

## GeniÅŸletme NoktalarÄ±

### 1. Yeni Veri KaynaÄŸÄ± Ekleme

```python
# scraper/sources/mynews.py
class MyNewsScraper(BaseScraper):
    def validate_config(self):
        # Validate
        return True
    
    def scrape(self, limit):
        # Custom logic
        pass

# scraper/scraper.py
from .sources.mynews import MyNewsScraper
ScraperRegistry.register('mynews', MyNewsScraper)

# config/config.yaml
sources:
  mynews:
    enabled: true
    api:
      base_url: "${MY_NEWS_API_URL}"
```

### 2. Yeni Ä°ÅŸleme ModÃ¼lÃ¼ Ekleme

```python
# processor/custom_processor.py
class CustomProcessor:
    def process(self, data):
        # Process data
        return processed_data

# cli.py
from processor.custom_processor import CustomProcessor
processor = CustomProcessor()
result = processor.process(data)
```

### 3. Yeni Export FormatÄ±

```python
# exporter/export_parquet.py
class ParquetExporter:
    def export(self, data, filename):
        # Export logic
        pass

# cli.py
from exporter.export_parquet import ParquetExporter
exporter = ParquetExporter()
exporter.export(data, "output.parquet")
```

## Best Practices

1. **Hardcoded deÄŸer kullanmayÄ±n**: Her ÅŸey config'de olmalÄ±
2. **Abstract class'lardan tÃ¼retin**: BaseScraper, BaseProcessor
3. **Registry pattern kullanÄ±n**: Yeni kaynaklar eklerken
4. **Environment variable'larÄ± tercih edin**: Hassas bilgiler iÃ§in
5. **Logging ekleyin**: TÃ¼m Ã¶nemli iÅŸlemler iÃ§in
6. **Error handling**: Try-catch bloklarÄ± ile

## KonfigÃ¼rasyon Ã–nceliÄŸi

1. **Environment Variables** (en yÃ¼ksek)
2. **config/config.yaml**
3. **VarsayÄ±lan deÄŸerler** (kod iÃ§inde)

## Ã–rnek KullanÄ±m

```python
from config import get_config
from scraper.scraper import Scraper
from processor.classify import TextClassifier

# Config'den oku
config = get_config()
if config.is_source_enabled('wikipedia'):
    # Scrape
    scraper = Scraper()
    data = scraper.scrape_source('wikipedia', limit=100)
    
    # Process
    classifier = TextClassifier()
    for record in data:
        record['category'] = classifier.classify(record['text'])
```

## Gelecek GeliÅŸmeler

- [ ] Database backend desteÄŸi
- [ ] Streaming data iÅŸleme
- [ ] Distributed scraping
- [ ] Plugin system
- [ ] Web UI
- [ ] API server

